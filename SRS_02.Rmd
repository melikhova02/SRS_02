---
title: "Математическое моделирование"
author: "Мелихова И.С."
date: '10 мая 2019 г '
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
  word_document: default
---
*Данные*: Online_Shopping_for_models.csv

*Модели*: SVM
 
Расчистаем основные параметры описательной статистики. 

```{r}
library('GGally')       # графики совместного разброса переменных
library('lmtest') # тесты остатков регрессионных моделей
library('FNN') # алгоритм kNN
library('mlbench')
library('ISLR')
library('e1071')     # SVM
library('ROCR')      # ROC-кривые
library('randomForest')      # случайный лес randomForest()
library('gbm')               # бустинг gbm()
library('tree') 
setwd("D:/Desktop")
DF <- read.table('Online_Shopping_for_models.csv', header = T,            # заголовок в первой строке
                 dec = ',',             # разделитель целой и дробной части
                 sep = ';')     # символы пропущенных значений
df <- na.omit(DF)
dim(df)
head(df)
str(df)
my.seed <- 12  
summary(df)
```



```{r}
# общее число наблюдений
n <- nrow(df)
# доля обучающей выборки
train.percent <- 0.5
# выбрать наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(n, n * train.percent)
train <- sample(n, n * train.percent)
```

##Деревья решений 

Построим дерево для категориального отклика Revenue.

```{r}
Revenue <- df$Revenue
tree.shopping <- tree(Revenue ~ ., df, subset = train)
summary(tree.shopping)
# визуализация
plot(tree.shopping)
text(tree.shopping, pretty = 0)
```


```{r}
yhat <- predict(tree.shopping, newdata = df[-train, ])
df.test <- df[-train, "Revenue"]
```

   

Рассмотрим более сложные методы улучшения качества дерева. Бэггинг -- частный случай случайного леса с $m = p$, поэтому и то, и другое можно построить функцией `randomForest()`.    

Для начала используем *бэггинг*, причём возьмём все 17 предикторов на каждом шаге (аргумент `mtry`).   

```{r, cache = T}
# бэггинг с 14 предикторами
df.test <- df[-train,]
set.seed(my.seed)
bag.df <- randomForest(Revenue ~ ., data = df, subset = train, 
                           mtry = 17, importance = TRUE)
bag.df
# прогноз
yhat.bag = predict(bag.df, newdata = df[-train, ])
yhat.bag
```
 
Можно изменить число деревьев с помощью аргумента `ntree`.   

```{r, cache = T}
# бэггинг с 13 предикторами и 25 деревьями
bag.df <- randomForest(Revenue ~ ., data = df, subset = train,
                           mtry = 16, ntree = 25)
# прогноз
yhat.bag <- predict(bag.df, newdata = df[-train, ])
yhat.bag
```

Теперь попробуем вырастить случайный лес. Берём 6 предикторов на каждом шаге.   

```{r, cache = T}
# обучаем модель
set.seed(my.seed)
rf.df <- randomForest(Revenue ~ ., data = df, subset = train,
                          mtry = 6, importance = TRUE)
# важность предикторов
importance(rf.df)  # оценки 
varImpPlot(rf.df)  # графики
```

По полученным данным можно сделать вывод о том, что наибольшее влияние в модели оказывают такие показатели, как PageValues и Administrative_Duration.

##Бустинг

Построим 5000 регрессионных деревьев с глубиной 4.

```{r}
library(gbm)
df$Weekend <- as.factor(df$Weekend)
set.seed(my.seed)
boost.df <- gbm(Revenue ~ ., data = df[train, ], distribution = "gaussian",
                    n.trees = 5000, interaction.depth = 4)
# график и таблица относительной важности переменных
summary(boost.df)
```

Теперь построим графики частной зависимости для двух наиболее важных предикторов: PageValues и Month. 

```{r}
par(mfrow = c(1, 2))
plot(boost.df, i = "PageValues")
```

```{r}
plot(boost.df, i = "Month")
```

Построим полученные модели, по лучшей сделаем прогноз на прогнозных данных, обучим модель SVM с различными формами ядерной функции.

**Модель 1**: $\hat{Revenue} = \hat{\beta}_0 + \hat{\beta}_1 \cdot PageValues+\hat{\beta}_2 \cdot Administrative_Duration$. 

``` {r}
# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(df)
# подгонка линейной модели на обучающей выборке
fit.lm <- lm(Revenue ~ PageValues +  Administrative_Duration, subset = inTrain)
# считаем MSE на тестовой выборке
mean((df$Revenue[-inTrain] - predict(fit.lm,
                              df[-inTrain, ]))^2)
# отсоединить таблицу с данными
detach(df)
```

**Модель 2**: $\hat{Revenue} = \hat{\beta}_0 + \hat{\beta}_1 \cdot PageValues+\hat{\beta}_2 \cdot Month$. 

``` {r}
# присоединить таблицу с данными: названия стоблцов будут доступны напрямую
attach(df)
# подгонка линейной модели на обучающей выборке
fit.lm <- lm(Revenue ~ PageValues +  Month, subset = inTrain)
# считаем MSE на тестовой выборке
mean((df$Revenue[-inTrain] - predict(fit.lm,
                              df[-inTrain, ]))^2)
# отсоединить таблицу с данными
detach(df)
```

Оба значения MSE оказалась минимальными, с незначительной разницей значение первой модели оказалось меньше, будем использовать эту модель для дальнейшей работы. 


##Машины опорных векторов

```{r}
# таблица с данными, отклик — фактор 
PageValues <- df$PageValues
Administrative_Duration <- df$Administrative_Duration
Revenue <- df$Revenue
Revenue <- as.factor(Revenue)
dat <- data.frame(PageValues, Administrative_Duration, Revenue)
# обучающая выборка 
train <- sample(1:nrow(dat), nrow(dat)/2)
# SVM с радиальным ядром и маленьким cost
svmfit <- svm(Revenue ~ ., data = dat[train, ], kernel = "radial", 
              gamma = 1, cost = 1)
plot(svmfit, dat[train, ])
summary(svmfit)
# SVM с радиальным ядром и большим cost
svmfit <- svm(Revenue ~ ., data = dat[train, ], kernel = "radial", 
              gamma = 1, cost = 1e5)
plot(svmfit, dat[train, ])
summary(svmfit)
```

Перекрестная проверка.

```{r}
# перекрёстная проверка
set.seed(my.seed)
tune.out <- tune(svm, Revenue ~ ., data = dat[train, ], kernel = "radial", 
                 ranges = list(cost = c(0.1, 1, 10, 5),
                               gamma = c(0.5, 0.1,0.05,1, 2, 3)))
summary(tune.out)
```

Построим матрицу неточностей для прогноза по лучшей модели на прогнозных данных и рассчитаем MSE.

```{r, message = F, warning = F, fig.height = 6, fig.width = 6}
setwd("D:/Desktop")
DF1 <- read.table('Online_Shopping_for_forecast.csv', header = T,            # заголовок в первой строке
                 dec = ',',             # разделитель целой и дробной части
                 sep = ';')     # символы пропущенных значений
df1 <- na.omit(DF1)
Revenue <- c("TRUE", "FALSE")
df1 <- data.frame(df1, Revenue) 
Revenue <- as.factor(Revenue)
Revenue <- df1$Revenue
Revenue <- Revenue[1:30]
PageValues <- df1$PageValues
PageValues <- PageValues[1:30]
Administrative_Duration <- df1$Administrative_Duration
Administrative_Duration <- Administrative_Duration[1:30]
n <- nrow(df1)
# доля обучающей выборки
train.percent <- 0.5
# выбрать наблюдения в обучающую выборку
set.seed(my.seed)
train <- sample(n, n * train.percent)
dat1 <- data.frame(PageValues, Administrative_Duration, Revenue)
#матрица неточностей для прогноза по лучшей модели
matrix <- table(true = dat1[-train, "Revenue"], 
      pred = predict(tune.out$best.model, newdata = dat1[-train, ]))
bestmod <- tune.out$best.model
summary(bestmod)
#MSE
sum(diag(matrix))/sum(matrix) 
```

MSE по лучшей модели на прогнозных данных составило- 0.4210526, значение оказалось очень маленьким, а значит предикторы в модели выбраны верно.
